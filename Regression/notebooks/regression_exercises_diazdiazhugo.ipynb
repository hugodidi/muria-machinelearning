{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729fb8d0",
   "metadata": {},
   "source": [
    "# Ejercicios del tema de Regresi√≥n\n",
    "\n",
    "*Hugo D√≠az D√≠az*(*hdiazdd00@estudiantes.unileon.es*)\n",
    "\n",
    "*Correo profesional: hugo.didi.contacto@gmail.com*\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<blockquote style=\"border-left: 4px solid #00ccff; padding: 0.5em;\">\n",
    "  <strong>üí° NOTA PARA EL PROFESORADO</strong> <br><br> <b>La entrega se realiza en un Jupyter Notebook ajeno al de los apuntes, primero por temas de ejecuci√≥n y montar mis propios flujos de datos desde cero y segundo por llevar un seguimiento y publicar mis ejercicios en Github, al que prefiero subir contenido exclusivamente propio. Espero que no sea una molestia y si as√≠ lo fuera lo cambiar√© para la pr√≥xima entrega.<br><br> Gracias.</b>\n",
    "</blockquote>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb7ad4",
   "metadata": {},
   "source": [
    "## Preguntas opcionales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb913d",
   "metadata": {},
   "source": [
    "### 1. ¬øQu√© algoritmo de entrenamiento de regresi√≥n lineal puede utilizar si dispone de un conjunto de entrenamiento con millones de caracter√≠sticas?\n",
    "\n",
    "---\n",
    "\n",
    "Con millones de caracter√≠sticas, entrenar√≠a con alguna variante de Gradient Descent (SGD o Mini-Batch). La Ecuaci√≥n Normal y la SVD se vuelven muy lentas cuando n crece mucho, mientras que GD escala bien con el n√∫mero de caracter√≠sticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36813b50",
   "metadata": {},
   "source": [
    "### 2. Suponga que las caracter√≠sticas de su conjunto de entrenamiento tienen escalas muy diferentes. ¬øQu√© algoritmos podr√≠an verse afectados por esta situaci√≥n? ¬øQu√© puede hacerse al respecto?\n",
    "\n",
    "---\n",
    "\n",
    "Si las variables est√°n en escalas muy distintas, los algoritmos basados en Gradient Descent se ven afectados (convergencia muy lenta o inestable). La soluci√≥n es escalar/estandarizar las caracter√≠sticas antes de entrenar (p. ej. con StandardScaler)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5b90d",
   "metadata": {},
   "source": [
    "### 3. ¬øPuede Gradient Descent quedarse atascado en un m√≠nimo local al entrenar un modelo de Regresi√≥n Log√≠stica?\n",
    "\n",
    "---\n",
    "\n",
    "En regresi√≥n log√≠stica (modelo lineal) la funci√≥n de p√©rdida (log-loss/entrop√≠a cruzada) es convexa respecto a los par√°metros, por lo que no existen m√≠nimos locales: cualquier m√≠nimo es global. Por eso GD no suele ‚Äúatascarse‚Äù en m√≠nimos locales; adem√°s, la naturaleza estoc√°stica de SGD ayuda a escapar de √≥ptimos ‚Äúmalos‚Äù si los hubiera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5fdb73",
   "metadata": {},
   "source": [
    "### 4. ¬øTodos los algoritmos Gradient Descent conducen al mismo modelo si se les deja correr el tiempo suficiente?\n",
    "\n",
    "---\n",
    "\n",
    "Tras suficiente entrenamiento, los m√©todos de GD acaban en modelos muy similares. Batch GD se detiene en el m√≠nimo; SGD y Mini-Batch se acercan y ‚Äúoscilan‚Äù alrededor, pero con un buen schedule de learning rate tambi√©n pueden asentarse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43e31f",
   "metadata": {},
   "source": [
    "### 5. Supongamos que utilizas Batch Gradient Descent y representas gr√°ficamente el error de validaci√≥n en cada epoch. Si observas que el error de validaci√≥n aumenta constantemente, ¬øqu√© puede estar pasando? ¬øC√≥mo puede solucionarlo?\n",
    "\n",
    "---\n",
    "\n",
    "Si el error de validaci√≥n sube en cada epoch con Batch GD, puede que est√©s sobreajustando o, m√°s t√≠picamente, que la tasa de aprendizaje sea demasiado alta y el entrenamiento est√© divergendo. Soluci√≥n: reduce Œ∑ y/o aplica regularizaci√≥n o parada temprana para cortar cuando el error de validaci√≥n deje de mejorar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273ebd4",
   "metadata": {},
   "source": [
    "### 6. ¬øEs una buena idea detener el Mini-Batch Gradient Descent inmediatamente cuando el error de validaci√≥n aumenta?\n",
    "\n",
    "---\n",
    "\n",
    "No conviene parar Mini-Batch GD en cuanto sube una vez el error de validaci√≥n: las curvas son ruidosas. Lo sensato es usar parada temprana con ‚Äúpaciencia‚Äù (esperar al m√≠nimo claro) o suavizar la m√©trica; entonces cortar en el m√≠nimo de validaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869f77d",
   "metadata": {},
   "source": [
    "### 7. ¬øQu√© algoritmo Gradient Descent (de entre los que hemos discutido) alcanzar√° m√°s r√°pidamente la proximidad de la soluci√≥n √≥ptima? ¬øCu√°l converger√° realmente? ¬øC√≥mo puedes hacer que los dem√°s tambi√©n converjan?\n",
    "\n",
    "---\n",
    "\n",
    "El que llega antes ‚Äúcerca‚Äù del √≥ptimo suele ser SGD (o Mini-Batch) porque cada actualizaci√≥n es barata. El que realmente converge y se queda en el m√≠nimo es Batch GD; a los otros puedes hacerlos converger usando un schedule que reduzca gradualmente el learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62197a36",
   "metadata": {},
   "source": [
    "### 8. Supongamos que se utiliza la regresi√≥n polin√≥mica. Se trazan las curvas de aprendizaje y se observa que hay una gran diferencia entre el error de entrenamiento y el error de validaci√≥n. ¬øQu√© ocurre? ¬øCu√°les son las tres formas de resolverlo?\n",
    "\n",
    "---\n",
    "\n",
    "Si con regresi√≥n polin√≥mica ves gran brecha entre error de entrenamiento (bajo) y de validaci√≥n (alto), hay sobreajuste. Tres v√≠as: reducir la complejidad (grado menor), a√±adir m√°s datos de entrenamiento y/o regularizar (Ridge/Lasso/Elastic Net)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa05405",
   "metadata": {},
   "source": [
    "### 9. Supongamos que se utiliza la regresi√≥n Ridge y se observa que el error de entrenamiento y el error de validaci√≥n son casi iguales y bastante elevados. ¬øDir√≠a que el modelo tiene un sesgo alto o una varianza alta? ¬øDeber√≠a aumentar el hiperpar√°metro de regularizaci√≥n Œ± o reducirlo?\n",
    "\n",
    "---\n",
    "\n",
    "Si en Ridge train y valid son casi iguales y altos, hay alto sesgo (underfitting). Para darle m√°s capacidad, reducir√≠a Œ± (demasiada regularizaci√≥n aplana el modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1f20e",
   "metadata": {},
   "source": [
    "### 10. ¬øPor qu√© querr√≠a utilizar...\n",
    "- Regresi√≥n Ridge en lugar de Regresi√≥n Lineal simple (es decir, sin ninguna regularizaci√≥n)?\n",
    "- Lasso en lugar de Ridge Regression?\n",
    "- Elastic Net en lugar de Lasso?\n",
    "\n",
    "---\n",
    "\n",
    "Usar√≠a Ridge en lugar de lineal ‚Äúpura‚Äù para controlar el sobreajuste manteniendo pesos peque√±os; Lasso en lugar de Ridge cuando prefiero penalizaci√≥n $l_1$; y Elastic Net cuando quiero un compromiso ajustable entre $l_1$ y $l_2$ (r mezcla ambos extremos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e02a1",
   "metadata": {},
   "source": [
    "### 11. Supongamos que se desea clasificar las im√°genes como exteriores/interiores y diurnas/nocturnas. ¬øDeber√≠a implementar dos clasificadores de Regresi√≥n Log√≠stica o un clasificador de Regresi√≥n Softmax?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Aqu√≠ son dos etiquetas independientes (exterior/interior y diurna/nocturna). Implementar√≠a dos clasificadores log√≠sticos binarios, no un Softmax, que est√° pensado para clases mutuamente excluyentes en un √∫nico eje multiclase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec2904",
   "metadata": {},
   "source": [
    "## Ejercicio obligatorio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff3c21",
   "metadata": {},
   "source": [
    "### 12. **(OBLIGATORIO)** Implementa Batch Gradient Descent con parada temprana para la Regresi√≥n Softmax (sin usar Scikit-Learn)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
